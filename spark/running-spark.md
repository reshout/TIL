# Learning Spark

## Chapter 1. 스파크를 이용한 데이터 분석 소개

### 스파크?

- **범용적이고 빠른 클러스터용 연산 플랫폼**
- 맵리듀스 모델을 대화형 명령어, 쿼리, 스트리밍 가능하도록 확장한 것
- 속도를 높이기 위해서 연산을 메모리에서 수행
- 고수준 파이썬, 자바, 스칼라, SQL API 내장
- 스칼라로 구현. JVM 위에서 동작.
- 하둡 클러스터 위에서 실행 가능
- 어떤 하둡 호환 데이터 소스에도 접근 가능 (예. HDFS, 로컬 파일 시스템, S3, 카산드라, 하이브, HBase)

### 구성

- 스파크 코어: 작업 스케줄링, 메모리 관리, 장애 복구, 저장장치와 연동 등 기본적인 기능 수행.
- 스파크 SQL: 정형 데이터를 처리하기 위한 스파크 패키지. SQL, 하이브 테이블, 피케이, JSON 등 다양한 데이터 소스 지원.
- 스파크 스트리밍: RDD API와 거의 일치하는 형태의 데이터 스트림 조작 API 지원
- MLlib: 일반적인 머신 러닝 기능 갖고 있는 라이브러리
- 그래프X: 그래프 병렬 연산 수행. 일반적 그래프 알고리즘 지원.
- 클러스터 매니저: 하둡의 얀, 아파치 메소스, 스파크 지원 단독 스케줄러 등 다양한 클러스터 매니저 위에서 동작.

## Chapter 2. 스파크 맛보기

### 다운로드

- `tgz` 파일 [다운로드](http://spark.apache.org/downloads.html) 및 압축해제
- 설치된 하둡 클러스터나 HDFS가 있다면 버전에 맞게 다운로드
- `bin` 디렉토리는 다양한 방식으로 스파크를 사용하기 위한 실행파일 포함
- `examples` 디렉토리는 스파크 API를 공부할 수 있는 간단한 예제코드 포함

### 대화형 셸

- 스파크는 즉석 데이터 분석이 가능한 대화형 셸 제공
- 대화형 셸은 파이썬과 스칼라만 가능

파이썬 셸은 아래와 같이 실행

```bash
bin/pyspark
```

셸을 빠져나가려면 Ctrl + D를 누른다.

IPython을 사용하려면 아래와 같이 실행

```bash
IPYTHON=1 ./bin/pyspark
```

파이썬 셸에서 로컬 파일로부터 RDD(Resilient Distributed Dataset)을 생성해 라인 수를 세는 방법은 아래와 같다.

```python
>>> lines = sc.textFile("../README.md")
>>> lines.count()
99
>>> lines.first()
u'# Apache Spark'
```
